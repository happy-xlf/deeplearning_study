{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Using the latest cached version of the dataset since YeungNLP/firefly-train-1.1M couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\xlf\\.cache\\huggingface\\datasets\\YeungNLP___firefly-train-1.1_m\\default\\0.0.0\\92947564f0b6bac44c405272df8cd7247937fc2d (last modified on Wed Oct 30 10:43:04 2024).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test_dataset = load_dataset(\"YeungNLP/firefly-train-1.1M\", split=\"train[:500]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['kind', 'input', 'target'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'ClassicalChinese',\n",
       " 'input': '我当时在三司，访求太祖、仁宗的手书敕令没有见到，然而人人能传诵那些话，禁止私盐的建议也最终被搁置。\\n翻译成文言文：',\n",
       " 'target': '余时在三司，求访两朝墨敕不获，然人人能诵其言，议亦竟寝。'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"F:\\llm_work\\Model\\Qwen2___5-0___5B-Instruct\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个人工智能助手，是up主“小明”开发的.\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"target\"]}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = test_dataset.map(format_prompt, remove_columns=test_dataset.column_names)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\n你是一个人工智能助手，是up主“小明”开发的.<|im_end|>\\n<|im_start|>user\\n自然语言推理：\\n前提：家里人心甘情愿地养他,还有几家想让他做女婿的\\n假设：他是被家里人收养的孤儿<|im_end|>\\n<|im_start|>assistant\\n中立<|im_end|>\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"F:\\llm_work\\Model\\Qwen2___5-0___5B-Instruct\").half()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"F:\\llm_work\\Model\\Qwen2___5-0___5B-Instruct\")\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = \"./outputs\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\me_soft\\Anaconda\\anaconda3\\envs\\xlf_pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "d:\\me_soft\\Anaconda\\anaconda3\\envs\\xlf_pytorch\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "d:\\me_soft\\Anaconda\\anaconda3\\envs\\xlf_pytorch\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046cef37115248e3b5ca0254cd5f970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9053, 'grad_norm': 2.084488868713379, 'learning_rate': 0.00019745268727865774, 'epoch': 0.08}\n",
      "{'loss': 3.2673, 'grad_norm': 2.499664783477783, 'learning_rate': 0.00018881364488135448, 'epoch': 0.16}\n",
      "{'loss': 3.0687, 'grad_norm': 1.2388889789581299, 'learning_rate': 0.00017459411454241822, 'epoch': 0.24}\n",
      "{'loss': 2.8596, 'grad_norm': 1.0219742059707642, 'learning_rate': 0.00015568756164881882, 'epoch': 0.32}\n",
      "{'loss': 2.6614, 'grad_norm': 1.2218854427337646, 'learning_rate': 0.00013328195445229868, 'epoch': 0.4}\n",
      "{'loss': 2.7552, 'grad_norm': 1.202169418334961, 'learning_rate': 0.00010878511965507434, 'epoch': 0.48}\n",
      "{'loss': 2.4015, 'grad_norm': 1.163712739944458, 'learning_rate': 8.373628348051165e-05, 'epoch': 0.56}\n",
      "{'loss': 2.5993, 'grad_norm': 1.6480128765106201, 'learning_rate': 5.9709356428633746e-05, 'epoch': 0.64}\n",
      "{'loss': 2.4824, 'grad_norm': 1.0274461507797241, 'learning_rate': 3.821403869096658e-05, 'epoch': 0.72}\n",
      "{'loss': 2.5851, 'grad_norm': 1.4728624820709229, 'learning_rate': 2.0600960135216462e-05, 'epoch': 0.8}\n",
      "{'loss': 2.5212, 'grad_norm': 1.0493898391723633, 'learning_rate': 7.976815263412963e-06, 'epoch': 0.88}\n",
      "{'loss': 2.5074, 'grad_norm': 1.3290791511535645, 'learning_rate': 1.134825526208605e-06, 'epoch': 0.96}\n",
      "{'train_runtime': 90.0084, 'train_samples_per_second': 5.555, 'train_steps_per_second': 1.389, 'train_loss': 2.7890047302246095, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\"./result/final_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./result/final_model\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个人工智能助手，是up主“小明”开发的.<|im_end|>\n",
      "<|im_start|>user\n",
      "我喜欢游泳，但不喜欢吃鱼。\n",
      "翻译成文言文：<|im_end|>\n",
      "<|im_start|>assistant\n",
      "吾喜游水，然恶食鱼。\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "\n",
    "prompt_example = \"\"\"<|im_start|>system\n",
    "你是一个人工智能助手，是up主“小明”开发的.<|im_end|>\n",
    "<|im_start|>user\n",
    "我喜欢游泳，但不喜欢吃鱼。\n",
    "翻译成文言文：<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "print(pipe(prompt_example, max_new_tokens=50)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个人工智能助手，是up主“小明”开发的.<|im_end|>\n",
      "<|im_start|>user\n",
      "我喜欢游泳，但不喜欢吃鱼。\n",
      "翻译成文言文：<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'吾喜泳而不厌，然好食鱼。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"我喜欢游泳，但不喜欢吃鱼。\\n翻译成文言文：\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个人工智能助手，是up主“小明”开发的.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlf_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
